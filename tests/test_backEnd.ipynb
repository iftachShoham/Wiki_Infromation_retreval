{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iCs7nhF7bKzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24724785-cbc5-45db-b18f-13001a3abf19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import io\n",
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import math\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from inverted_index_gcp import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "!pip install -q google-cloud-storage==1.43.0"
      ],
      "metadata": {
        "id": "NObA9KK3bUAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9888ef8a-7db7-484c-a90b-6dee7b85d7da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.43.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is if i want to add spell checking....\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "metadata": {
        "id": "YJapmdncpiND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f311335-75e3-4723-cfe9-10bb895dcf66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "riF1qPIqbT-f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading"
      ],
      "metadata": {
        "id": "tywzyFyQPij8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreadSafeCounter(Counter):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        with self.lock:\n",
        "            super().__setitem__(key, value)\n"
      ],
      "metadata": {
        "id": "H-vaA7GbQR5C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BackEnd():\n",
        "  \"\"\"\n",
        "  Class that handle all of the backEnd of the project.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "\n",
        "    # Number of docs in the corpus\n",
        "    self.N = 6348910\n",
        "\n",
        "    # For inverted index of the text alone:\n",
        "    self.bucket_name_text = \"bucket_ir_102\"\n",
        "    self.base_dir_inverted = \"postings_gcp\"\n",
        "    self.text_InvertedIndex = InvertedIndex.read_index(self.base_dir_inverted, \"index\", self.bucket_name_text)\n",
        "\n",
        "    # For inverted index of the title alone:\n",
        "    self.bucket_name_title = \"bucket_ir_101\"\n",
        "    self.title_InvertedIndex = InvertedIndex.read_index(self.base_dir_inverted, \"index_title\", self.bucket_name_title)\n",
        "\n",
        "    # For Page rank dict:\n",
        "    bucket_name_for_page_rank = \"bucket_ir_100\"\n",
        "    file_name_for_page_rank = \"pr/part-00000-bba051bd-4ed5-42d2-ac51-b81e7da0af95-c000.csv.gz\"\n",
        "    self.page_rank_dict = self.read_csv_gzip_to_dict(bucket_name_for_page_rank, file_name_for_page_rank)\n",
        "\n",
        "    # For title_id:\n",
        "    bucket_name_for_title_id = \"bucket_ir_100\"\n",
        "    file_name_for_title_id = \"id_title/part-00000-037780c9-c08d-4f8b-92db-c059071f2db8-c000.csv.gz\"\n",
        "    self.title_id = self.read_csv_gzip_to_dict_with_string(bucket_name_for_title_id, file_name_for_title_id)\n",
        "\n",
        "    # For doc_legnth dict:\n",
        "    bucket_name_for_doc_len = \"bucket_ir_100\"\n",
        "    file_name_for_doc_len = \"word_counts/part-00000-1487dca0-a7d2-4dd0-b547-440beeb5f720-c000.csv.gz\"\n",
        "    self.doc_length_dict = self.read_csv_gzip_to_dict(bucket_name_for_doc_len, file_name_for_doc_len)\n",
        "\n",
        "\n",
        "    # For stemming\n",
        "    english_stopwords = frozenset(stopwords.words('english'))\n",
        "    corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "    self.all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "    self.RE_WORD = re.compile(r\"\\b\\w{2,24}\\b\", re.UNICODE)\n",
        "\n",
        "\n",
        "  def read_csv_gzip_to_dict(self, bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Reads csv from the bucket, given the file name into a dict and returns it.\n",
        "    key = doc_id\n",
        "    value = page rank\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the file as bytes\n",
        "    content = blob.download_as_string()\n",
        "\n",
        "    # Decompress the gzip file and read it as a CSV\n",
        "    with gzip.open(io.BytesIO(content), \"rt\") as gzip_file:\n",
        "        csv_reader = csv.reader(gzip_file)\n",
        "        header = next(csv_reader)  # Assuming the first row is the header\n",
        "        data = {int(row[0]): float(row[1]) for row in csv_reader}\n",
        "\n",
        "    return data\n",
        "\n",
        "  def read_csv_gzip_to_dict_with_string(self, bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Reads csv from the bucket, given the file name into a dict and returns it.\n",
        "    key = doc_id\n",
        "    value = title\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the file as bytes\n",
        "    content = blob.download_as_string()\n",
        "\n",
        "    # Decompress the gzip file and read it as a CSV\n",
        "    with gzip.open(io.BytesIO(content), \"rt\") as gzip_file:\n",
        "        csv_reader = csv.reader(gzip_file)\n",
        "        header = next(csv_reader)  # Assuming the first row is the header\n",
        "        data = {}\n",
        "        for row in csv_reader:\n",
        "            try:\n",
        "                doc_id = int(row[1])\n",
        "            except ValueError:\n",
        "                # Skip rows where doc_id is not a valid integer\n",
        "                continue\n",
        "            data[doc_id] = row[0]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "  def tf_idf(self, query, tf_idf_dict):\n",
        "    \"\"\"\n",
        "    args:\n",
        "    query - stemmed and trimmed list of the words that were searched.\n",
        "    tf_idf_dict- counter\n",
        "\n",
        "    returns a counter:\n",
        "      key = doc_id\n",
        "      value = W (int) calculated by the tf-idf\n",
        "      with the words asked.\n",
        "    \"\"\"\n",
        "\n",
        "    for word in query:\n",
        "        # Only handle words that are in the dictionary.\n",
        "        if word in self.text_InvertedIndex.df:\n",
        "            term_df = self.text_InvertedIndex.df[word]\n",
        "            pl = self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "            # Map it to get -> [(doc_id,W)....]\n",
        "            pl_map = map(lambda x: (x[0],\n",
        "                        (x[1]/self.doc_length_dict.get(x[0],1))*(math.log2(self.N/term_df))), pl)\n",
        "\n",
        "            # Update tf_idf_dict with the new values, summing if the key already exists\n",
        "            tf_idf_dict.update(dict(pl_map))\n",
        "\n",
        "    return tf_idf_dict\n",
        "\n",
        "\n",
        "  def BM25(self, query, BM25_dict):\n",
        "    \"\"\"\n",
        "  args:\n",
        "  query - stemmed and trimmed list of the words that were searched.\n",
        "  BM25_dict- counter\n",
        "\n",
        "  returns a counter:\n",
        "    key = doc_id\n",
        "    value = W (int) calculated by the tf-idf\n",
        "    with the words asked.\n",
        "  \"\"\"\n",
        "    # Create a defaultdict to store the cosine similarity of each document to the query\n",
        "    similarities = defaultdict(int)\n",
        "    # Putting back the query to sentence and create a counter of the query terms\n",
        "    counter_query = Counter(\" \".join(query))\n",
        "    for word in query:\n",
        "      if word in self.text_InvertedIndex.df:\n",
        "        term_df = self.text_InvertedIndex.df[word]\n",
        "        # Calculate the inverse document frequency (IDF) for the term\n",
        "        idf = math.log2(self.N/term_df)\n",
        "        # Read the posting list for the term\n",
        "        pl = self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "        print(pl)\n",
        "        # For each document where the term appears, calculate the term frequency (TF)\n",
        "        # and weight it by the IDF\n",
        "        for doc_id, freq in pl:\n",
        "          tf = freq / self.doc_length_dict.get(doc_id, 2000)\n",
        "          weight = tf * idf\n",
        "          # Add the weighted term to the document's similarity score\n",
        "          similarities[doc_id] += weight * counter_query[word]\n",
        "\n",
        "    print(list(similarities.values())[:20])\n",
        "\n",
        "\n",
        "\n",
        "    # Normalize the similarity score by the query and document lengths\n",
        "    normalization_query = 0\n",
        "    sum_q = 0\n",
        "\n",
        "    # Calculate the length of the query vector\n",
        "    for term, freq in counter_query.items():\n",
        "        sum_q += freq * freq\n",
        "    normalization_query = 1 / math.sqrt(sum_q)\n",
        "\n",
        "    # For each document, normalize the similarity score by the document length and the query length\n",
        "    for doc_id in similarities.keys():\n",
        "        nf = 1 / math.sqrt(self.doc_length_dict.get(doc_id, 2000))\n",
        "        similarities[doc_id] *= normalization_query * nf\n",
        "\n",
        "    # Update the dict\n",
        "    BM25_dict.update(similarities)\n",
        "\n",
        "\n",
        "  def page_rank_refactoring(self, tf_idf_dict, factor_value_pr=1):\n",
        "    \"\"\"\n",
        "    After the tf-idf has been done, it will refactor the result\n",
        "    with Page rank.\n",
        "    \"\"\"\n",
        "    for k in tf_idf_dict.keys():\n",
        "      tf_idf_dict[k] = factor_value_pr * self.page_rank_dict.get(k, 0)\n",
        "\n",
        "\n",
        "\n",
        "  def title_refactoring(self, query, tf_idf_dict, factor_value_title=1):\n",
        "    \"\"\"\n",
        "    After the tf-idf has been done, it will refactor the result\n",
        "    with title factoring, assuming if the words were in the title it\n",
        "    should have more weight.\n",
        "    \"\"\"\n",
        "    for word in query:\n",
        "    # Only handle words that are in the dictionary.\n",
        "      if word in self.title_InvertedIndex.df:\n",
        "        term_df = self.title_InvertedIndex.df[word]\n",
        "        pl = self.title_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_title)\n",
        "        # Map it to get -> [(doc_id,W)....]\n",
        "        pl_map = map(lambda x: (x[0],factor_value_title * max((math.log2(1+(self.N/term_df))),1)),pl)\n",
        "        # Update tf_idf_dict with the new values, summing if the key already exists\n",
        "        tf_idf_dict.update(dict(pl_map))\n",
        "\n",
        "\n",
        "\n",
        "  def filter_query(self, query):\n",
        "    \"\"\"\n",
        "    This will recive a raw string of the query and stemm it,\n",
        "    remove stopping words and creating a list of each token in it.\n",
        "    returning the list.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = re.findall(self.RE_WORD, query.lower())\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in stemmed_tokens if token not in self.all_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "  def filter_query_title(self, query):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = re.findall(self.RE_WORD, query.lower())\n",
        "    filtered_query = [word for word in word_tokens if word.lower() not in self.all_stopwords]\n",
        "    return filtered_query\n",
        "\n",
        "\n",
        "  # def correct_typos(self, text):\n",
        "  #   \"\"\"\n",
        "  #   Corrects typos for text.\n",
        "  #   \"\"\"\n",
        "  #   spell = SpellChecker()\n",
        "  #   words = text.split()\n",
        "  #   corrected_words = [spell.correction(word) for word in words]\n",
        "  #   return ' '.join(corrected_words)\n",
        "\n",
        "\n",
        "  def question_wiki(self, query, use_title=True,use_page_rank=True,TITLE_VALUE_FACTORING=1,PAGE_RANK_FACTORING=1,use_multy_thread=True):\n",
        "    \"\"\"\n",
        "    Going through the entire process, given a string of the question asked,\n",
        "    it will return the 100 most fitted wiki_id_pages and its title.\n",
        "    \"\"\"\n",
        "\n",
        "    # # Step 1: correct typos.\n",
        "    # query = self.correct_typos(query)\n",
        "    original_q = self.filter_query_title(query)\n",
        "\n",
        "    # Step 2: filter and stem\n",
        "    query = self.filter_query(query)\n",
        "\n",
        "    # Step 3: tf_idf on the body\n",
        "    # Now its multy threaded.\n",
        "    threads = []\n",
        "    tf_idf_dict = ThreadSafeCounter()\n",
        "\n",
        "    if use_multy_thread:\n",
        "      for q in query:\n",
        "          thread = threading.Thread(target=self.BM25, args=([q], tf_idf_dict))\n",
        "          threads.append(thread)\n",
        "          thread.start()\n",
        "\n",
        "    # Step 3: tf_idf on the body not multy threaded\n",
        "    else:\n",
        "      self.BM25(query,tf_idf_dict)\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: use the title factoring\n",
        "    if use_title and not use_multy_thread:\n",
        "      self.title_refactoring(original_q, tf_idf_dict, TITLE_VALUE_FACTORING)\n",
        "\n",
        "    # Use the title factoring not multy threaded\n",
        "    if use_title and use_multy_thread:\n",
        "      for q in original_q:\n",
        "          thread = threading.Thread(target=self.title_refactoring, args=([q], tf_idf_dict,TITLE_VALUE_FACTORING))\n",
        "          threads.append(thread)\n",
        "          thread.start()\n",
        "\n",
        "    # Make sure to join all of the threads before the pagerank.\n",
        "    if use_multy_thread:\n",
        "      for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    # Step 5: use the page rank refactoring\n",
        "    if use_page_rank:\n",
        "      self.page_rank_refactoring(tf_idf_dict, PAGE_RANK_FACTORING)\n",
        "\n",
        "    # Step 6: Return top 100 results.\n",
        "\n",
        "    # Convert the Counter to a list of tuples and sort it by value in descending order\n",
        "    sorted_counter = sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Get the top 100 items\n",
        "    top_100_keys = [(doc_id,self.title_id[doc_id],value) for doc_id, value in sorted_counter[:100]]\n",
        "\n",
        "    return top_100_keys\n",
        "\n",
        "\n",
        "\n",
        "  def get_posting_list_body(self, word):\n",
        "    \"\"\"\n",
        "    return the posting list of a word by the text index\n",
        "    \"\"\"\n",
        "    return self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "\n",
        "  def get_posting_list_title(self, word):\n",
        "    \"\"\"\n",
        "    return the posting list of a word by the title index\n",
        "    \"\"\"\n",
        "    return self.title_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_title)\n",
        "\n",
        "  def get_page_rank(self, doc_id):\n",
        "    \"\"\"\n",
        "    returns the page rank by the doc id.\n",
        "    \"\"\"\n",
        "    return self.page_rank_dict.get(doc_id, 0)\n",
        "\n",
        "  def get_doc_length(self, doc_id):\n",
        "    # Returns the doc_length by the doc id.\n",
        "    return self.doc_length_dict.get(doc_id,1)\n"
      ],
      "metadata": {
        "id": "kFrYbWMcbT8d"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing for indexing"
      ],
      "metadata": {
        "id": "eYC5t2XAvToB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the backEnd\n",
        "import time\n",
        "start_time = time.time()\n",
        "be = BackEnd()\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "assert elapsed_time < 100\n",
        "# Note, shouldnt take more than 100 secs"
      ],
      "metadata": {
        "id": "bsBQjhzGvlVg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con = Counter()\n",
        "test = be.title_refactoring([\"computer\"],con)\n",
        "print(list(con.items())[:10])\n",
        "\n",
        "test = be.title_refactoring([\"computer\"],con)\n",
        "\n",
        "print(list(con.items())[:10])\n"
      ],
      "metadata": {
        "id": "vilNgjMytWMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for bodyIndexing\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "pl = be.get_posting_list_body(\"computer\")\n",
        "\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "for i in range(10):\n",
        "  print(pl[i])\n",
        "\n",
        "print(\" \")\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n"
      ],
      "metadata": {
        "id": "gTkjmNcMbT6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for titleIndexing\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "pl = be.get_posting_list_title(\"computer\")\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "for i in range(30):\n",
        "  print(pl[i])\n",
        "\n",
        "print(\" \")\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "4MmWceA4bT4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for page rank indexing\n",
        "\n",
        "doc_id = 2428\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "print(be.get_page_rank(doc_id))\n",
        "\n",
        "print(\" \")\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "CKONLZIIvgiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for doc legth\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "print(be.get_doc_length(doc_id))\n",
        "\n",
        "print(\" \")\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "03nNo81Rvgf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a4d847-78d9-46b1-f2d0-f371dd594a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3375.0\n",
            " \n",
            "Elapsed time: 0.0002732276916503906 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for typos correction\n",
        "\n",
        "text_with_typos = \"ths is an exmple of text with typos\"\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "corrected_text = be.correct_typos(text_with_typos)\n",
        "\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "print(\" \")\n",
        "assert \"the is an example of text with typos\" ==  corrected_text\n",
        "\n",
        "# Took aprox 0.17 secs\n"
      ],
      "metadata": {
        "id": "L6Vys8FJvgWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for filter_query\n",
        "\n",
        "query = \"the cat is on the mat\"\n",
        "expected_tokens = [\"cat\", \"mat\"]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "ans =  be.filter_query(query)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "print(ans)\n",
        "assert expected_tokens ==  ans\n"
      ],
      "metadata": {
        "id": "dJS68VePwiOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for tf_idf\n",
        "start_time = time.time()\n",
        "ans = be.tf_idf([\"computer\"])\n",
        "elapsed_time = time.time() - start_time\n",
        "print(len(ans))\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "\n",
        "# assert len(ans) == 134110\n",
        "assert elapsed_time < 1.5\n"
      ],
      "metadata": {
        "id": "XXJM46TUwqie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for id_title\n",
        "title = be.title_id[18555]\n",
        "print(title)"
      ],
      "metadata": {
        "id": "9poT8_ZnNncg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for question_wiki\n",
        "start_time = time.time()\n",
        "\n",
        "ans = be.question_wiki(\"computer\",use_title=False, use_page_rank=False, TITLE_VALUE_FACTORING=1000)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time With multy thread: {elapsed_time} seconds\")\n",
        "\n",
        "# start_time = time.time()\n",
        "\n",
        "# ans2 = be.question_wiki(\"computer violet flower\",use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000,use_multy_thread=False)\n",
        "\n",
        "# elapsed_time = time.time() - start_time\n",
        "\n",
        "# print(f\"Elapsed time Without multy thread: {elapsed_time} seconds\")\n",
        "\n",
        "# assert ans1==ans2\n",
        "\n",
        "\n",
        "assert len(ans) == 100\n",
        "for i in ans:\n",
        "  print(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZP9ooRSwtQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_list = [\n",
        "    \"What is the history of the Roman Empire?\",\n",
        "    \"How does the human brain process language?\",\n",
        "    \"Who were the key figures in the Renaissance?\",\n",
        "    \"What are the main causes of climate change?\",\n",
        "    \"How does the Internet work?\",\n",
        "    \"What is the theory of relativity?\",\n",
        "    \"Who was Cleopatra and what was her impact on history?\",\n",
        "    \"What are the major events of World War II?\",\n",
        "    \"How does the human digestive system work?\",\n",
        "    \"Who were the Vikings and what were their achievements?\",\n",
        "    \"What is the significance of the Magna Carta?\",\n",
        "    \"How do vaccines work?\",\n",
        "    \"Who was Joan of Arc and what role did she play in history?\",\n",
        "    \"What are the major religions of the world?\",\n",
        "    \"What is the process of photosynthesis?\",\n",
        "    \"Who were the ancient Egyptians and what was their culture like?\",\n",
        "    \"What is the history of the United States Constitution?\",\n",
        "    \"How does the human immune system work?\",\n",
        "    \"Who was Alexander the Great and what were his conquests?\",\n",
        "    \"What are the major theories of evolution?\",\n",
        "    \"How does the global economy work?\",\n",
        "    \"What are the major accomplishments of the Industrial Revolution?\",\n",
        "    \"Who was Leonardo da Vinci and what were his contributions to art and science?\",\n",
        "    \"What are the major theories of the origin of life on Earth?\",\n",
        "    \"How does the human respiratory system work?\",\n",
        "    \"Who were the key figures in the Civil Rights Movement?\",\n",
        "    \"What is the history of the Olympic Games?\",\n",
        "    \"How does the human nervous system work?\",\n",
        "    \"Who was Julius Caesar and what were his accomplishments?\",\n",
        "    \"What are the major branches of philosophy?\",\n",
        "    \"How does the human reproductive system work?\",\n",
        "    \"Who were the key figures in the American Revolution?\",\n",
        "    \"What is the history of the space race?\",\n",
        "    \"How does the human cardiovascular system work?\",\n",
        "    \"Who was Genghis Khan and what was his impact on history?\",\n",
        "    \"What are the major theories of the origin of the universe?\",\n",
        "    \"How does the criminal justice system work?\",\n",
        "    \"Who were the key figures in the French Revolution?\",\n",
        "    \"What is the history of the United Nations?\",\n",
        "    \"How does the human endocrine system work?\",\n",
        "    \"Who was Attila the Hun and what was his impact on history?\",\n",
        "    \"What are the major theories of the formation of the solar system?\",\n",
        "    \"How does the human skeletal system work?\",\n",
        "    \"Who were the key figures in the women's suffrage movement?\",\n",
        "    \"What is the history of the European Union?\",\n",
        "    \"How does the human muscular system work?\",\n",
        "    \"Who was Napoleon Bonaparte and what were his accomplishments?\",\n",
        "    \"What are the major theories of the extinction of the dinosaurs?\",\n",
        "    \"How does the human lymphatic system work?\",\n",
        "    \"Who were the key figures in the abolitionist movement?\"\n",
        "]"
      ],
      "metadata": {
        "id": "_pP5GSFhaOhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_no_thread = []\n",
        "ans_with_thread = []"
      ],
      "metadata": {
        "id": "uZUb1MfjbD-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in questions_list:\n",
        "  start_time = time.time()\n",
        "  ans1 = be.question_wiki(q,use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  ans_no_thread.append(elapsed_time)\n",
        "  start_time = time.time()\n",
        "  ans2 = be.question_wiki(q,use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000,use_multy_thread=False)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  ans_with_thread.append(elapsed_time)\n"
      ],
      "metadata": {
        "id": "G1U2AIJmbD4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_no_thread"
      ],
      "metadata": {
        "id": "b26TNyZobhA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_with_thread"
      ],
      "metadata": {
        "id": "ju1ArESnbjAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have two lists of results, result1 and result2, and question_indices as before\n",
        "result1 = ans_no_thread\n",
        "result2 = ans_with_thread\n",
        "\n",
        "# Create a figure and axis with a wider size\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "\n",
        "# Define bar width and indices for the two sets of results\n",
        "bar_width = 0.35\n",
        "indices = np.arange(len(questions_list))\n",
        "question_lengths = [len(q.split()) for q in questions_list]  # Split question and count words\n",
        "\n",
        "# Create grouped bar plot\n",
        "bar1 = ax.bar(indices - bar_width/2, result1, bar_width, label='Single thread', color='b')\n",
        "bar2 = ax.bar(indices + bar_width/2, result2, bar_width, label='Multy thread', color='g')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Number of Words in Question')\n",
        "ax.set_ylabel('Time (s)')\n",
        "ax.set_title('Time Taken to Process Questions of Different Word Counts')\n",
        "\n",
        "# Set x-axis ticks and labels\n",
        "ax.set_xticks(indices)\n",
        "ax.set_xticklabels(question_lengths)\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6RVxaZ4Sf8lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming ans_no_thread and ans_with_thread are the two lists of values\n",
        "def avg_percentage_difference(list1, list2):\n",
        "    differences = [(a - b) / ((a + b) / 2) * 100 for a, b in zip(list1, list2)]\n",
        "    return sum(differences) / len(differences)\n",
        "\n",
        "avg_diff = avg_percentage_difference(ans_no_thread, ans_with_thread)\n",
        "print(f\"Average Percentage Difference: {avg_diff:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcRmjRE-jbLx",
        "outputId": "ee61cd1c-266a-45e9-b4ff-457f05a07f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Percentage Difference: -37.12%\n"
          ]
        }
      ]
    }
  ]
}