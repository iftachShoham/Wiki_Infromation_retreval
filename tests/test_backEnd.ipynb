{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iCs7nhF7bKzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3257ef-20b5-4806-8f66-63919d7c9e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import io\n",
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import math\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from inverted_index_gcp import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "!pip install -q google-cloud-storage==1.43.0"
      ],
      "metadata": {
        "id": "NObA9KK3bUAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is if i want to add spell checking....\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "metadata": {
        "id": "YJapmdncpiND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "riF1qPIqbT-f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading"
      ],
      "metadata": {
        "id": "tywzyFyQPij8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreadSafeCounter(Counter):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        with self.lock:\n",
        "            super().__setitem__(key, value)\n"
      ],
      "metadata": {
        "id": "H-vaA7GbQR5C"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BackEnd():\n",
        "  \"\"\"\n",
        "  Class that handle all of the backEnd of the project.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "\n",
        "    # Number of docs in the corpus\n",
        "    self.N = 6348910\n",
        "\n",
        "    # For inverted index of the text alone:\n",
        "    self.bucket_name_text = \"bucket_ir_102\"\n",
        "    self.base_dir_inverted = \"postings_gcp\"\n",
        "    self.text_InvertedIndex = InvertedIndex.read_index(self.base_dir_inverted, \"index\", self.bucket_name_text)\n",
        "\n",
        "    # For inverted index of the title alone:\n",
        "    self.bucket_name_title = \"bucket_ir_101\"\n",
        "    self.title_InvertedIndex = InvertedIndex.read_index(self.base_dir_inverted, \"index_title\", self.bucket_name_title)\n",
        "\n",
        "    # For Page rank dict:\n",
        "    bucket_name_for_page_rank = \"bucket_ir_100\"\n",
        "    file_name_for_page_rank = \"pr/part-00000-bba051bd-4ed5-42d2-ac51-b81e7da0af95-c000.csv.gz\"\n",
        "    self.page_rank_dict = self.read_csv_gzip_to_dict(bucket_name_for_page_rank, file_name_for_page_rank)\n",
        "\n",
        "    # For title_id:\n",
        "    bucket_name_for_title_id = \"bucket_ir_100\"\n",
        "    file_name_for_title_id = \"id_title/part-00000-037780c9-c08d-4f8b-92db-c059071f2db8-c000.csv.gz\"\n",
        "    self.title_id = self.read_csv_gzip_to_dict_with_string(bucket_name_for_title_id, file_name_for_title_id)\n",
        "\n",
        "    # For doc_legnth dict:\n",
        "    bucket_name_for_doc_len = \"bucket_ir_100\"\n",
        "    file_name_for_doc_len = \"word_counts/part-00000-1487dca0-a7d2-4dd0-b547-440beeb5f720-c000.csv.gz\"\n",
        "    self.doc_length_dict = self.read_csv_gzip_to_dict(bucket_name_for_doc_len, file_name_for_doc_len)\n",
        "\n",
        "\n",
        "    # For stemming\n",
        "    english_stopwords = frozenset(stopwords.words('english'))\n",
        "    corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "    self.all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "    self.RE_WORD = re.compile(r\"\\b\\w{2,24}\\b\", re.UNICODE)\n",
        "\n",
        "\n",
        "  def read_csv_gzip_to_dict(self, bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Reads csv from the bucket, given the file name into a dict and returns it.\n",
        "    key = doc_id\n",
        "    value = page rank\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the file as bytes\n",
        "    content = blob.download_as_string()\n",
        "\n",
        "    # Decompress the gzip file and read it as a CSV\n",
        "    with gzip.open(io.BytesIO(content), \"rt\") as gzip_file:\n",
        "        csv_reader = csv.reader(gzip_file)\n",
        "        header = next(csv_reader)  # Assuming the first row is the header\n",
        "        data = {int(row[0]): float(row[1]) for row in csv_reader}\n",
        "\n",
        "    return data\n",
        "\n",
        "  def read_csv_gzip_to_dict_with_string(self, bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Reads csv from the bucket, given the file name into a dict and returns it.\n",
        "    key = doc_id\n",
        "    value = title\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the file as bytes\n",
        "    content = blob.download_as_string()\n",
        "\n",
        "    # Decompress the gzip file and read it as a CSV\n",
        "    with gzip.open(io.BytesIO(content), \"rt\") as gzip_file:\n",
        "        csv_reader = csv.reader(gzip_file)\n",
        "        header = next(csv_reader)  # Assuming the first row is the header\n",
        "        data = {}\n",
        "        for row in csv_reader:\n",
        "            try:\n",
        "                doc_id = int(row[1])\n",
        "            except ValueError:\n",
        "                # Skip rows where doc_id is not a valid integer\n",
        "                continue\n",
        "            data[doc_id] = row[0]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "  def tf_idf(self, query, tf_idf_dict):\n",
        "    \"\"\"\n",
        "    args:\n",
        "    query - stemmed and trimmed list of the words that were searched.\n",
        "    tf_idf_dict- counter\n",
        "\n",
        "    returns a counter:\n",
        "      key = doc_id\n",
        "      value = W (int) calculated by the tf-idf\n",
        "      with the words asked.\n",
        "    \"\"\"\n",
        "\n",
        "    for word in query:\n",
        "        # Only handle words that are in the dictionary.\n",
        "        if word in self.text_InvertedIndex.df:\n",
        "            term_df = self.text_InvertedIndex.df[word]\n",
        "            pl = self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "            # Map it to get -> [(doc_id,W)....]\n",
        "            pl_map = map(lambda x: (x[0],\n",
        "                        (x[1]/self.doc_length_dict.get(x[0],1))*(math.log2(self.N/term_df))), pl)\n",
        "\n",
        "            # Update tf_idf_dict with the new values, summing if the key already exists\n",
        "            tf_idf_dict.update(dict(pl_map))\n",
        "\n",
        "    return tf_idf_dict\n",
        "\n",
        "\n",
        "\n",
        "  def page_rank_refactoring(self, tf_idf_dict, factor_value_pr=1):\n",
        "    \"\"\"\n",
        "    After the tf-idf has been done, it will refactor the result\n",
        "    with Page rank.\n",
        "    \"\"\"\n",
        "    for k in tf_idf_dict.keys():\n",
        "      tf_idf_dict[k] = factor_value_pr * self.page_rank_dict.get(k, 0)\n",
        "\n",
        "\n",
        "\n",
        "  def title_refactoring(self, query, tf_idf_dict, factor_value_title=1):\n",
        "    \"\"\"\n",
        "    After the tf-idf has been done, it will refactor the result\n",
        "    with title factoring, assuming if the words were in the title it\n",
        "    should have more weight.\n",
        "    \"\"\"\n",
        "    for word in query:\n",
        "    # Only handle words that are in the dictionary.\n",
        "      if word in self.title_InvertedIndex.df:\n",
        "        term_df = self.title_InvertedIndex.df[word]\n",
        "        pl = self.title_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_title)\n",
        "        # Map it to get -> [(doc_id,W)....]\n",
        "        pl_map = map(lambda x: (x[0],factor_value_title * max((math.log2(1+(self.N/term_df))),1)),pl)\n",
        "        # Update tf_idf_dict with the new values, summing if the key already exists\n",
        "        tf_idf_dict.update(dict(pl_map))\n",
        "\n",
        "\n",
        "\n",
        "  def filter_query(self, query):\n",
        "    \"\"\"\n",
        "    This will recive a raw string of the query and stemm it,\n",
        "    remove stopping words and creating a list of each token in it.\n",
        "    returning the list.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = re.findall(self.RE_WORD, query.lower())\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in stemmed_tokens if token not in self.all_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "  def filter_query_title(self, query):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = re.findall(self.RE_WORD, query.lower())\n",
        "    filtered_query = [word for word in word_tokens if word.lower() not in self.all_stopwords]\n",
        "    return filtered_query\n",
        "\n",
        "\n",
        "  # def correct_typos(self, text):\n",
        "  #   \"\"\"\n",
        "  #   Corrects typos for text.\n",
        "  #   \"\"\"\n",
        "  #   spell = SpellChecker()\n",
        "  #   words = text.split()\n",
        "  #   corrected_words = [spell.correction(word) for word in words]\n",
        "  #   return ' '.join(corrected_words)\n",
        "\n",
        "\n",
        "  def question_wiki(self, query, use_title=True,use_page_rank=True,TITLE_VALUE_FACTORING=1,PAGE_RANK_FACTORING=1,use_multy_thread=True):\n",
        "    \"\"\"\n",
        "    Going through the entire process, given a string of the question asked,\n",
        "    it will return the 100 most fitted wiki_id_pages and its title.\n",
        "    \"\"\"\n",
        "\n",
        "    # # Step 1: correct typos.\n",
        "    # query = self.correct_typos(query)\n",
        "    original_q = self.filter_query_title(query)\n",
        "\n",
        "    # Step 2: filter and stem\n",
        "    query = self.filter_query(query)\n",
        "\n",
        "    # Step 3: tf_idf on the body\n",
        "    # Now its multy threaded.\n",
        "    threads = []\n",
        "    tf_idf_dict = ThreadSafeCounter()\n",
        "\n",
        "    if use_multy_thread:\n",
        "      for q in query:\n",
        "          thread = threading.Thread(target=self.tf_idf, args=([q], tf_idf_dict))\n",
        "          threads.append(thread)\n",
        "          thread.start()\n",
        "\n",
        "    # Step 3: tf_idf on the body not multy threaded\n",
        "    else:\n",
        "      self.tf_idf(query,tf_idf_dict)\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: use the title factoring\n",
        "    if use_title and not use_multy_thread:\n",
        "      self.title_refactoring(original_q, tf_idf_dict, TITLE_VALUE_FACTORING)\n",
        "\n",
        "    # Use the title factoring not multy threaded\n",
        "    if use_title and use_multy_thread:\n",
        "      for q in original_q:\n",
        "          thread = threading.Thread(target=self.title_refactoring, args=([q], tf_idf_dict,TITLE_VALUE_FACTORING))\n",
        "          threads.append(thread)\n",
        "          thread.start()\n",
        "\n",
        "    # Make sure to join all of the threads before the pagerank.\n",
        "    if use_multy_thread:\n",
        "      for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    # Step 5: use the page rank refactoring\n",
        "    if use_page_rank:\n",
        "      self.page_rank_refactoring(tf_idf_dict, PAGE_RANK_FACTORING)\n",
        "\n",
        "    # Step 6: Return top 100 results.\n",
        "\n",
        "    # Convert the Counter to a list of tuples and sort it by value in descending order\n",
        "    sorted_counter = sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Get the top 100 items\n",
        "    top_100_keys = [(doc_id,self.title_id[doc_id],value) for doc_id, value in sorted_counter[:100]]\n",
        "\n",
        "    return top_100_keys\n",
        "\n",
        "\n",
        "\n",
        "  def get_posting_list_body(self, word):\n",
        "    \"\"\"\n",
        "    return the posting list of a word by the text index\n",
        "    \"\"\"\n",
        "    return self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "\n",
        "  def get_posting_list_title(self, word):\n",
        "    \"\"\"\n",
        "    return the posting list of a word by the title index\n",
        "    \"\"\"\n",
        "    return self.title_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_title)\n",
        "\n",
        "  def get_page_rank(self, doc_id):\n",
        "    \"\"\"\n",
        "    returns the page rank by the doc id.\n",
        "    \"\"\"\n",
        "    return self.page_rank_dict.get(doc_id, 0)\n",
        "\n",
        "  def get_doc_length(self, doc_id):\n",
        "    # Returns the doc_length by the doc id.\n",
        "    return self.doc_length_dict.get(doc_id,1)\n"
      ],
      "metadata": {
        "id": "kFrYbWMcbT8d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing for indexing"
      ],
      "metadata": {
        "id": "eYC5t2XAvToB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the backEnd\n",
        "import time\n",
        "start_time = time.time()\n",
        "be = BackEnd()\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "assert elapsed_time < 100\n",
        "# Note, shouldnt take more than 100 secs"
      ],
      "metadata": {
        "id": "bsBQjhzGvlVg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con = Counter()\n",
        "test = be.title_refactoring([\"computer\"],con)\n",
        "print(list(con.items())[:10])\n",
        "\n",
        "test = be.title_refactoring([\"computer\"],con)\n",
        "\n",
        "print(list(con.items())[:10])\n"
      ],
      "metadata": {
        "id": "vilNgjMytWMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for bodyIndexing\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "pl = be.get_posting_list_body(\"computer\")\n",
        "\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "for i in range(10):\n",
        "  print(pl[i])\n",
        "\n",
        "print(\" \")\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n"
      ],
      "metadata": {
        "id": "gTkjmNcMbT6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for titleIndexing\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "pl = be.get_posting_list_title(\"computer\")\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "for i in range(30):\n",
        "  print(pl[i])\n",
        "\n",
        "print(\" \")\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "4MmWceA4bT4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for page rank indexing\n",
        "\n",
        "doc_id = 2428\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "print(be.get_page_rank(doc_id))\n",
        "\n",
        "print(\" \")\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "CKONLZIIvgiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc29b85-ff87-43f0-eb15-a693a2839a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29.24096506722234\n",
            " \n",
            "Elapsed time: 0.00029969215393066406 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for doc legth\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "print(be.get_doc_length(doc_id))\n",
        "\n",
        "print(\" \")\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "03nNo81Rvgf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a4d847-78d9-46b1-f2d0-f371dd594a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3375.0\n",
            " \n",
            "Elapsed time: 0.0002732276916503906 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for typos correction\n",
        "\n",
        "text_with_typos = \"ths is an exmple of text with typos\"\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "corrected_text = be.correct_typos(text_with_typos)\n",
        "\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "print(\" \")\n",
        "assert \"the is an example of text with typos\" ==  corrected_text\n",
        "\n",
        "# Took aprox 0.17 secs\n"
      ],
      "metadata": {
        "id": "L6Vys8FJvgWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for filter_query\n",
        "\n",
        "query = \"the cat is on the mat\"\n",
        "expected_tokens = [\"cat\", \"mat\"]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "ans =  be.filter_query(query)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "print(ans)\n",
        "assert expected_tokens ==  ans\n"
      ],
      "metadata": {
        "id": "dJS68VePwiOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for tf_idf\n",
        "start_time = time.time()\n",
        "ans = be.tf_idf([\"computer\"])\n",
        "elapsed_time = time.time() - start_time\n",
        "print(len(ans))\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "\n",
        "# assert len(ans) == 134110\n",
        "assert elapsed_time < 1.5\n"
      ],
      "metadata": {
        "id": "XXJM46TUwqie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for id_title\n",
        "title = be.title_id[18555]\n",
        "print(title)"
      ],
      "metadata": {
        "id": "9poT8_ZnNncg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for question_wiki\n",
        "start_time = time.time()\n",
        "\n",
        "ans = be.question_wiki(\"computer violet flower\",use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time With multy thread: {elapsed_time} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "ans = be.question_wiki(\"computer violet flower\",use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000,use_multy_thread=False)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time Without multy thread: {elapsed_time} seconds\")\n",
        "\n",
        "\n",
        "assert len(ans) == 100\n",
        "# for i in ans:\n",
        "#   print(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZP9ooRSwtQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}