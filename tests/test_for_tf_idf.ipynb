{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjINW1uJMJtV"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import io\n",
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import math\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from inverted_index_gcp import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "!pip install -q google-cloud-storage==1.43.0"
      ],
      "metadata": {
        "id": "eUMyLq6jMPl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "WeaarnVSMPkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"bucket_ir_102\"\n",
        "base_dir = \"postings_gcp\"\n",
        "N = 6348910 # Number of docs in the corpus\n",
        "\n",
        "# Reading the inverted index.\n",
        "invertedIndex = InvertedIndex.read_index(base_dir,\"index\",bucket_name)"
      ],
      "metadata": {
        "id": "ZT9Kl6vgMPiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(invertedIndex, doc_length_dict, query, bucket_name, base_dir=\"postings_gcp\"):\n",
        "    \"\"\"\n",
        "    args:\n",
        "    invertedIndex - inverted index, which allows to get all of the data needed.\n",
        "    doc_length_dict - a dict that when given the doc_id, gives the length of it.\n",
        "    query - stemmed and trimmed list of the words that were searched.\n",
        "    bucket_name - the name of the bucket connected to the inverted index.\n",
        "\n",
        "    returns a counter:\n",
        "      key = doc_id\n",
        "      value = W calculated by the tf-idf\n",
        "      with the words asked.\n",
        "    \"\"\"\n",
        "    N = 6348910 # Number of docs in the corpus\n",
        "\n",
        "    # For the return values\n",
        "    tf_idf_dict = Counter()\n",
        "\n",
        "    for word in query:\n",
        "        # Only handle words that are in the dictionary.\n",
        "        if word in invertedIndex.df:\n",
        "            term_df = invertedIndex.df[word]\n",
        "            pl = invertedIndex.read_a_posting_list(base_dir, word, bucket_name)\n",
        "            # Map it to get -> [(doc_id,W)....]\n",
        "            pl_map = pl.map(lambda x: (x[0],\n",
        "                            (x[1]/doc_length_dict[x[0]])*(math.log2(N/term_df))))\n",
        "            # Update tf_idf_dict with the new values, summing if the key already exists\n",
        "            tf_idf_dict.update(dict(pl_map))\n",
        "\n",
        "    return tf_idf_dict"
      ],
      "metadata": {
        "id": "LcUFVIw_MPf4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}