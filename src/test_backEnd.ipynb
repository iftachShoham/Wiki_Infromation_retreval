{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCs7nhF7bKzg"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import io\n",
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import math\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from inverted_index_gcp import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "!pip install -q google-cloud-storage==1.43.0"
      ],
      "metadata": {
        "id": "NObA9KK3bUAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07535649-9348-447a-d37c-9099ad9ea3cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m92.2/106.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.43.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is if i want to add spell checking....\n",
        "# !pip install pyspellchecker\n",
        "# from spellchecker import SpellChecker"
      ],
      "metadata": {
        "id": "YJapmdncpiND"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "import os\n",
        "# Set the environment variable to point to the uploaded JSON file\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"myproject-413806-ee8f72ba2e37.json\"\n",
        "\n",
        "# You can now use the Google Cloud Client Libraries to interact with Google services\n",
        "# For example, if you're using Google Cloud Storage:\n",
        "from google.cloud import storage\n",
        "\n",
        "# Create a client using the authenticated credentials\n",
        "client = storage.Client()"
      ],
      "metadata": {
        "id": "riF1qPIqbT-f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading"
      ],
      "metadata": {
        "id": "tywzyFyQPij8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreadSafeCounter(Counter):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        with self.lock:\n",
        "            super().__setitem__(key, value)\n"
      ],
      "metadata": {
        "id": "H-vaA7GbQR5C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from LLMImprove import *\n",
        "\n",
        "class BackEnd():\n",
        "  \"\"\"\n",
        "  Class that handle all of the backEnd of the project.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "\n",
        "    # Number of docs in the corpus\n",
        "    self.N = 6348910\n",
        "\n",
        "    # For bm25:\n",
        "    self.k1 = 1.0\n",
        "    self.b = 0.9\n",
        "    self.k2 = 10.0\n",
        "\n",
        "    # For inverted index of the text alone:\n",
        "    self.bucket_name_text = \"bucket_ir_102\"\n",
        "    self.base_dir_inverted = \"postings_gcp\"\n",
        "    self.text_InvertedIndex = InvertedIndex.read_index(self.base_dir_inverted, \"index\", self.bucket_name_text)\n",
        "\n",
        "    # For inverted index of the title alone:\n",
        "    self.bucket_name_title = \"bucket_ir_101\"\n",
        "    self.title_InvertedIndex = InvertedIndex.read_index(self.base_dir_inverted, \"index_title\", self.bucket_name_title)\n",
        "\n",
        "    # For Page rank dict:\n",
        "    bucket_name_for_page_rank = \"bucket_ir_100\"\n",
        "    file_name_for_page_rank = \"pr/part-00000-bba051bd-4ed5-42d2-ac51-b81e7da0af95-c000.csv.gz\"\n",
        "    self.page_rank_dict = self.read_csv_gzip_to_dict(bucket_name_for_page_rank, file_name_for_page_rank)\n",
        "\n",
        "    # For title_id:\n",
        "    bucket_name_for_title_id = \"bucket_ir_100\"\n",
        "    file_name_for_title_id = \"id_title/part-00000-037780c9-c08d-4f8b-92db-c059071f2db8-c000.csv.gz\"\n",
        "    self.title_id = self.read_csv_gzip_to_dict_with_string(bucket_name_for_title_id, file_name_for_title_id)\n",
        "\n",
        "    # For doc_legnth dict:\n",
        "    bucket_name_for_doc_len = \"bucket_ir_100\"\n",
        "    file_name_for_doc_len = \"word_counts/part-00000-1487dca0-a7d2-4dd0-b547-440beeb5f720-c000.csv.gz\"\n",
        "    self.doc_length_dict = self.read_csv_gzip_to_dict(bucket_name_for_doc_len, file_name_for_doc_len)\n",
        "\n",
        "    self.avg_doc_length = 0\n",
        "    for length in self.doc_length_dict.values():\n",
        "      self.avg_doc_length += length\n",
        "\n",
        "    self.avg_doc_length = self.avg_doc_length / len(self.doc_length_dict)  # Assuming self.N is the number of documents\n",
        "\n",
        "\n",
        "    # For stemming\n",
        "    english_stopwords = frozenset(stopwords.words('english'))\n",
        "    corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "    self.all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "    self.RE_WORD = re.compile(r\"\\b\\w{2,24}\\b\", re.UNICODE)\n",
        "\n",
        "    self.question_words = {\n",
        "                        'who', 'what', 'when', 'where', 'why', 'how', 'which', 'whom', 'whose',\n",
        "                        'are', 'is', 'do', 'does', 'did', 'can', 'could', 'would', 'should',\n",
        "                        'may', 'might', 'shall', 'must', 'have', 'has', 'had', 'am', 'was', 'were',\n",
        "                        'had', 'have', 'having', 'been', 'being', 'if', 'whether', 'whenever', 'wherever',\n",
        "                        'whatever', 'whichever', 'whoever', 'whomever', 'whyever', '?'}\n",
        "\n",
        "\n",
        "  def read_csv_gzip_to_dict(self, bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Reads csv from the bucket, given the file name into a dict and returns it.\n",
        "    key = doc_id\n",
        "    value = page rank\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the file as bytes\n",
        "    content = blob.download_as_string()\n",
        "\n",
        "    # Decompress the gzip file and read it as a CSV\n",
        "    with gzip.open(io.BytesIO(content), \"rt\") as gzip_file:\n",
        "        csv_reader = csv.reader(gzip_file)\n",
        "        header = next(csv_reader)  # Assuming the first row is the header\n",
        "        data = {int(row[0]): float(row[1]) for row in csv_reader}\n",
        "\n",
        "    return data\n",
        "\n",
        "  def read_csv_gzip_to_dict_with_string(self, bucket_name, file_name):\n",
        "    \"\"\"\n",
        "    Reads csv from the bucket, given the file name into a dict and returns it.\n",
        "    key = doc_id\n",
        "    value = title\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the file as bytes\n",
        "    content = blob.download_as_string()\n",
        "\n",
        "    # Decompress the gzip file and read it as a CSV\n",
        "    with gzip.open(io.BytesIO(content), \"rt\") as gzip_file:\n",
        "      csv_reader = csv.reader(gzip_file)\n",
        "      header = next(csv_reader)  # Assuming the first row is the header\n",
        "      data = {}\n",
        "      for row in csv_reader:\n",
        "        try:\n",
        "          doc_id = int(row[1])\n",
        "        except ValueError:\n",
        "          # Skip rows where doc_id is not a valid integer\n",
        "          continue\n",
        "        data[doc_id] = row[0]\n",
        "\n",
        "    return data\n",
        "\n",
        "  def bm25(self, query, tf_idf_dict):\n",
        "    # create an empty Counter object to store document scores\n",
        "    candidates = Counter()\n",
        "\n",
        "    # loop through each term in the query\n",
        "    for word in query:\n",
        "      # check if the word exists in the corpus\n",
        "      if word in self.text_InvertedIndex.df:\n",
        "        # read the posting list of the word\n",
        "        posting_list = self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "\n",
        "        # calculate idf of the word\n",
        "        df = self.text_InvertedIndex.df[word]\n",
        "        idf = math.log(1 + (self.N - df + 0.5) / (df + 0.5))\n",
        "\n",
        "        # loop through each (doc_id, freq) pair in the posting list\n",
        "        for doc_id, freq in posting_list:\n",
        "\n",
        "          len_doc = self.doc_length_dict.get(doc_id, -1)\n",
        "          if len_doc == -1:\n",
        "            continue\n",
        "\n",
        "          # calculate bm25 score of the word for the document\n",
        "          numerator = idf * freq * (self.k1 + 1)\n",
        "          denominator = (freq + self.k1 * (1 - self.b + self.b * len_doc / self.avg_doc_length))\n",
        "          bm25_score = numerator / denominator\n",
        "          bm25_score = bm25_score * ((self.k2 + 1) * freq / (self.k2 + freq))\n",
        "\n",
        "          # add the bm25 score to the document's score in the candidates Counter\n",
        "          candidates[doc_id] += bm25_score\n",
        "    tf_idf_dict.update(candidates)\n",
        "\n",
        "  # def tf_idf(self, query, tf_idf_dict):\n",
        "  #   \"\"\"\n",
        "  #   args:\n",
        "  #   query - stemmed and trimmed list of the words that were searched.\n",
        "  #   tf_idf_dict- counter\n",
        "  #\n",
        "  #   returns a counter:\n",
        "  #     key = doc_id\n",
        "  #     value = W (int) calculated by the tf-idf\n",
        "  #     with the words asked.\n",
        "  #   \"\"\"\n",
        "  #\n",
        "  #   for word in query:\n",
        "  #       # Only handle words that are in the dictionary.\n",
        "  #       if word in self.text_InvertedIndex.df:\n",
        "  #           term_df = self.text_InvertedIndex.df[word]\n",
        "  #           pl = self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "  #           # Map it to get -> [(doc_id,W)....]\n",
        "  #           pl_map = map(lambda x: (x[0],\n",
        "  #                       (x[1]/self.doc_length_dict.get(x[0],1))*(math.log2(self.N/term_df))), pl)\n",
        "  #\n",
        "  #           # Update tf_idf_dict with the new values, summing if the key already exists\n",
        "  #           tf_idf_dict.update(dict(pl_map))\n",
        "  #\n",
        "  #   return tf_idf_dict\n",
        "\n",
        "  def tf_idf(self, query, tf_idf_dict):\n",
        "    \"\"\"\n",
        "  args:\n",
        "  query - stemmed and trimmed list of the words that were searched.\n",
        "  BM25_dict- counter\n",
        "\n",
        "  returns a counter:\n",
        "    key = doc_id\n",
        "    value = W (int) calculated by the tf-idf\n",
        "    with the words asked.\n",
        "  \"\"\"\n",
        "    # Create a defaultdict to store the cosine similarity of each document to the query\n",
        "    similarities = defaultdict(int)\n",
        "    # Putting back the query to sentence and create a counter of the query terms\n",
        "    counter_query = Counter(query)\n",
        "    for word in query:\n",
        "      if word in self.text_InvertedIndex.df:\n",
        "        term_df = self.text_InvertedIndex.df[word]\n",
        "        # Calculate the inverse document frequency (IDF) for the term\n",
        "        idf = math.log2(self.N / term_df)\n",
        "        # Read the posting list for the term\n",
        "        pl = self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "        # For each document where the term appears, calculate the term frequency (TF)\n",
        "        # and weight it by the IDF\n",
        "        for doc_id, freq in pl:\n",
        "          tf = freq / self.doc_length_dict.get(doc_id, 2000)\n",
        "          weight = tf * idf\n",
        "          # Add the weighted term to the document's similarity score\n",
        "          similarities[doc_id] += weight * counter_query[word]\n",
        "\n",
        "    # Normalize the similarity score by the query and document lengths\n",
        "    normalization_query = 0\n",
        "    sum_q = 0\n",
        "\n",
        "    # Calculate the length of the query vector\n",
        "    for term, freq in counter_query.items():\n",
        "      sum_q += freq * freq\n",
        "    normalization_query = 1 / math.sqrt(sum_q)\n",
        "\n",
        "    # For each document, normalize the similarity score by the document length and the query length\n",
        "    for doc_id in similarities.keys():\n",
        "      nf = 1 / math.sqrt(self.doc_length_dict.get(doc_id, 2000))\n",
        "      similarities[doc_id] *= normalization_query * nf\n",
        "\n",
        "    # Update the dict\n",
        "    tf_idf_dict.update(similarities)\n",
        "\n",
        "\n",
        "  def page_rank_refactoring(self, tf_idf_dict, factor_value_pr=1):\n",
        "    \"\"\"\n",
        "    After the tf-idf has been done, it will refactor the result\n",
        "    with Page rank.\n",
        "    \"\"\"\n",
        "    for k in tf_idf_dict.keys():\n",
        "      tf_idf_dict[k] += math.log2(factor_value_pr * self.page_rank_dict.get(k, 1))\n",
        "\n",
        "\n",
        "\n",
        "  def title_refactoring(self, query, tf_idf_dict, factor_value_title=1):\n",
        "      \"\"\"\n",
        "      After the tf-idf has been done, it will refactor the result\n",
        "      with title factoring, assuming if the words were in the title it\n",
        "      should have more weight.\n",
        "      \"\"\"\n",
        "      for word in query:\n",
        "      # Only handle words that are in the dictionary.\n",
        "        if word in self.title_InvertedIndex.df:\n",
        "          term_df = self.title_InvertedIndex.df[word]\n",
        "          pl = self.title_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_title)\n",
        "          # Map it to get -> [(doc_id,W)....]\n",
        "          pl_map = map(lambda x: (x[0], factor_value_title * (1 /max(1,len(self.title_id.get(x[0], [])))) * max((math.log2(1 + (self.N / term_df))), 1)), pl)\n",
        "          # Update tf_idf_dict with the new values, summing if the key already exists\n",
        "          tf_idf_dict.update(dict(pl_map))\n",
        "\n",
        "\n",
        "  def filter_query_with_stemming(self, query):\n",
        "    \"\"\"\n",
        "    This will recive a raw string of the query and stemm it,\n",
        "    remove stopping words and creating a list of each token in it.\n",
        "    returning the list.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = re.findall(self.RE_WORD, query.lower())\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in stemmed_tokens if token not in self.all_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "  def filter_query_title(self, query):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = re.findall(self.RE_WORD, query.lower())\n",
        "    filtered_query = [word for word in word_tokens if word.lower() not in self.all_stopwords]\n",
        "    return filtered_query\n",
        "\n",
        "  def filter_query_without_stemming(self, query):\n",
        "    \"\"\"\n",
        "    This will recive a raw string of the query and stemm it,\n",
        "    remove stopping words and creating a list of each token in it.\n",
        "    returning the list.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(self.RE_WORD, query.lower())\n",
        "    filtered_tokens = [token for token in tokens if token not in self.all_stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "  def question_wiki(self, query, use_title=True,use_page_rank=True,TITLE_VALUE_FACTORING=1,PAGE_RANK_FACTORING=1,use_multy_thread=True):\n",
        "    \"\"\"\n",
        "    Going through the entire process, given a string of the question asked,\n",
        "    it will return the 100 most fitted wiki_id_pages and its title.\n",
        "    \"\"\"\n",
        "    # Checking special condition\n",
        "    # if '\"' in query:\n",
        "    #   return self.question_wiki_with_par(query,use_title=use_title,use_page_rank=use_page_rank,TITLE_VALUE_FACTORING=TITLE_VALUE_FACTORING,PAGE_RANK_FACTORING=PAGE_RANK_FACTORING)\n",
        "\n",
        "    # Using llm to improve the query\n",
        "\n",
        "    llm_q = LLMImprove.improve_query(query)\n",
        "    # # Step 1: correct typos.\n",
        "    # query = self.correct_typos(query)\n",
        "    original_q = self.filter_query_title(llm_q)\n",
        "\n",
        "    if \"when\" in query or \"When\" in query:\n",
        "      search_method = self.bm25\n",
        "    else:\n",
        "      search_method = self.tf_idf\n",
        "      query = llm_q\n",
        "\n",
        "\n",
        "    # flag_for_question = True\n",
        "    # for word in original_q:\n",
        "    #   if word in self.question_words:\n",
        "    #     PAGE_RANK_FACTORING = PAGE_RANK_FACTORING * 250\n",
        "    #     flag_for_question = False\n",
        "    #     break\n",
        "\n",
        "    # # Use regular expression to find individual words within double quotes\n",
        "    # words_within_quotes = re.findall(r'\"([^\"]*)\"', query)\n",
        "    # # Join the words into a sentence\n",
        "    # sentence_within_quotes = ' '.join(words_within_quotes)\n",
        "    # # query = self.correct_typos(query)\n",
        "    # list_of_words_within_quotes = self.filter_query_with_stemming(sentence_within_quotes)\n",
        "\n",
        "\n",
        "    # Step 2: filter and stem and if the query length is less than 3, so also add the unstemmed words.\n",
        "    if len(query) < 3:\n",
        "        query = self.filter_query_without_stemming(query) + self.filter_query_with_stemming(query)\n",
        "        TITLE_VALUE_FACTORING = TITLE_VALUE_FACTORING * 2\n",
        "    else:\n",
        "        query = self.filter_query_with_stemming(query)\n",
        "\n",
        "    # step 3: check if their question words in the query\n",
        "\n",
        "\n",
        "    # Step 4: tf_idf on the body\n",
        "    # Now its multy threaded.\n",
        "    threads = []\n",
        "    tf_idf_dict = ThreadSafeCounter()\n",
        "\n",
        "    if use_multy_thread:\n",
        "      for q in query:\n",
        "          thread = threading.Thread(target=search_method, args=([q], tf_idf_dict))\n",
        "          threads.append(thread)\n",
        "          thread.start()\n",
        "\n",
        "    # Step 4: tf_idf on the body not multy threaded\n",
        "    else:\n",
        "      search_method(query, tf_idf_dict)\n",
        "\n",
        "\n",
        "    # Step 5: use the title factoring\n",
        "    if use_title and not use_multy_thread:\n",
        "      self.title_refactoring(original_q, tf_idf_dict, TITLE_VALUE_FACTORING)\n",
        "\n",
        "    # Use the title factoring not multy threaded\n",
        "    if use_title and use_multy_thread:\n",
        "      for q in original_q:\n",
        "          thread = threading.Thread(target=self.title_refactoring, args=([q], tf_idf_dict, TITLE_VALUE_FACTORING))\n",
        "          threads.append(thread)\n",
        "          thread.start()\n",
        "\n",
        "    # Make sure to join all of the threads before the pagerank.\n",
        "    if use_multy_thread:\n",
        "      for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    # Checking to see if the dict is too small, if so ill lower the stemming\n",
        "    if len(tf_idf_dict) < 100:\n",
        "      threads = []\n",
        "      if use_multy_thread:\n",
        "        for q in original_q:\n",
        "          thread = threading.Thread(target=search_method, args=([q], tf_idf_dict))\n",
        "          threads.append(thread)\n",
        "          thread.start()\n",
        "        for thread in threads:\n",
        "          thread.join()\n",
        "\n",
        "      # Step 3: tf_idf on the body not multy threaded\n",
        "      else:\n",
        "        search_method(original_q, tf_idf_dict)\n",
        "\n",
        "\n",
        "    # Step 5: use the page rank refactoring\n",
        "    if use_page_rank:\n",
        "      self.page_rank_refactoring(tf_idf_dict, PAGE_RANK_FACTORING)\n",
        "\n",
        "    # Step 6: Return top 100 results.\n",
        "\n",
        "    # Convert the Counter to a list of tuples and sort it by value in descending order\n",
        "    sorted_counter = sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Get the top 100 items\n",
        "    top_100_keys = [\n",
        "      (str(doc_id), self.title_id[doc_id])\n",
        "      for doc_id, value in sorted_counter[:min(100, len(sorted_counter))]\n",
        "      if doc_id in self.title_id\n",
        "    ]\n",
        "\n",
        "    return top_100_keys\n",
        "\n",
        "  # def question_wiki_with_par(self, query, use_title=True,use_page_rank=True,TITLE_VALUE_FACTORING=1,PAGE_RANK_FACTORING=1,use_multy_thread=True):\n",
        "  #   # Getting the words insied the \"\"\n",
        "  #   pattern = r'\"(.*?)\"'\n",
        "  #   query = re.findall(pattern, query)\n",
        "\n",
        "  #   query = LLMImprove.improve_query(\" \".join(query))\n",
        "\n",
        "  #   # Steming\n",
        "  #   query = self.filter_query_with_stemming(query)\n",
        "\n",
        "  #   similarities = defaultdict(int)\n",
        "  #   # Putting back the query to sentence and create a counter of the query terms\n",
        "  #   counter_query = Counter(query)\n",
        "  #   flag_first = True\n",
        "\n",
        "  #   for word in query:\n",
        "  #     if word in self.text_InvertedIndex.df:\n",
        "  #       term_df = self.text_InvertedIndex.df[word]\n",
        "  #       # Calculate the inverse document frequency (IDF) for the term\n",
        "  #       idf = math.log2(self.N / term_df)\n",
        "  #       # Read the posting list for the term\n",
        "  #       pl = self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "  #       for doc_id, freq in pl:\n",
        "  #         # Making sure the words already exist\n",
        "  #         if doc_id not in similarities or flag_first:\n",
        "  #           tf = freq / self.doc_length_dict.get(doc_id, 2000)\n",
        "  #           weight = tf * idf\n",
        "  #           # Add the weighted term to the document's similarity score\n",
        "  #           similarities[doc_id] += weight * counter_query[word]\n",
        "\n",
        "  #       flag_first = False\n",
        "  #   # Normalize the similarity score by the query and document lengths\n",
        "  #   normalization_query = 0\n",
        "  #   sum_q = 0\n",
        "\n",
        "  #   # Calculate the length of the query vector\n",
        "  #   for term, freq in counter_query.items():\n",
        "  #     sum_q += freq * freq\n",
        "  #   normalization_query = 1 / math.sqrt(sum_q)\n",
        "\n",
        "  #   # For each document, normalize the similarity score by the document length and the query length\n",
        "  #   for doc_id in similarities.keys():\n",
        "  #     nf = 1 / math.sqrt(self.doc_length_dict.get(doc_id, 2000))\n",
        "  #     similarities[doc_id] *= normalization_query * nf\n",
        "\n",
        "\n",
        "  #   if use_page_rank:\n",
        "  #     self.page_rank_refactoring(similarities, PAGE_RANK_FACTORING)\n",
        "\n",
        "  #   # Convert the Counter to a list of tuples and sort it by value in descending order\n",
        "  #   sorted_counter = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "  #   # Get the top 100 items\n",
        "  #   top_100_keys = [\n",
        "  #     (str(doc_id), self.title_id[doc_id])\n",
        "  #     for doc_id, value in sorted_counter[:min(100, len(sorted_counter))]\n",
        "  #     if doc_id in self.title_id\n",
        "  #   ]\n",
        "  #   return top_100_keys\n",
        "\n",
        "\n",
        "\n",
        "  def get_posting_list_body(self, word):\n",
        "    \"\"\"\n",
        "    return the posting list of a word by the text index\n",
        "    \"\"\"\n",
        "    return self.text_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_text)\n",
        "\n",
        "  def get_posting_list_title(self, word):\n",
        "    \"\"\"\n",
        "    return the posting list of a word by the title index\n",
        "    \"\"\"\n",
        "    return self.title_InvertedIndex.read_a_posting_list(self.base_dir_inverted, word, self.bucket_name_title)\n",
        "\n",
        "  def get_page_rank(self, doc_id):\n",
        "    \"\"\"\n",
        "    returns the page rank by the doc id.\n",
        "    \"\"\"\n",
        "    return self.page_rank_dict.get(doc_id, 0)\n",
        "\n",
        "  def get_doc_length(self, doc_id):\n",
        "    # Returns the doc_length by the doc id.\n",
        "    return self.doc_length_dict.get(doc_id,1)\n",
        "\n",
        "class ThreadSafeCounter(Counter):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lock = threading.Lock()\n",
        "\n",
        "  def __setitem__(self, key, value):\n",
        "    with self.lock:\n",
        "      super().__setitem__(key, value)\n"
      ],
      "metadata": {
        "id": "kFrYbWMcbT8d"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing for indexing"
      ],
      "metadata": {
        "id": "eYC5t2XAvToB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the backEnd\n",
        "import time\n",
        "start_time = time.time()\n",
        "be = BackEnd()\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "assert elapsed_time < 100\n",
        "# Note, shouldnt take more than 100 secs"
      ],
      "metadata": {
        "id": "bsBQjhzGvlVg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('queries_train.json', 'rt') as f:\n",
        "  queries = json.load(f)"
      ],
      "metadata": {
        "id": "FIHuOg5Rodp0"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_precision(true_list, predicted_list, k=40):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    precisions = []\n",
        "    for i,doc_id in enumerate(predicted_list):\n",
        "        if doc_id in true_set:\n",
        "            prec = (len(precisions)+1) / (i+1)\n",
        "            precisions.append(prec)\n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    return round(sum(precisions)/len(precisions),3)"
      ],
      "metadata": {
        "id": "CU-7C3JuogvT"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(predicted_list) == 0:\n",
        "        return 0.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(predicted_list), 3)\n",
        "def recall_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(true_set) < 1:\n",
        "        return 1.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(true_set), 3)\n",
        "def f1_at_k(true_list, predicted_list, k):\n",
        "    p = precision_at_k(true_list, predicted_list, k)\n",
        "    r = recall_at_k(true_list, predicted_list, k)\n",
        "    if p == 0.0 or r == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p + 1.0/r), 3)\n",
        "def results_quality(true_list, predicted_list):\n",
        "    p5 = precision_at_k(true_list, predicted_list, 5)\n",
        "    f1_30 = f1_at_k(true_list, predicted_list, 30)\n",
        "    if p5 == 0.0 or f1_30 == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p5 + 1.0/f1_30), 3)\n",
        "\n",
        "assert precision_at_k(range(10), [1,2,3] , 2) == 1.0\n",
        "assert recall_at_k(   range(10), [10,5,3], 2) == 0.1\n",
        "assert precision_at_k(range(10), []      , 2) == 0.0\n",
        "assert precision_at_k([],        [1,2,3],  5) == 0.0\n",
        "assert recall_at_k(   [],        [10,5,3], 2) == 1.0\n",
        "assert recall_at_k(   range(10), [],       2) == 0.0\n",
        "assert f1_at_k(       [],        [1,2,3],  5) == 0.0\n",
        "assert f1_at_k(       range(10), [],       2) == 0.0\n",
        "assert f1_at_k(       range(10), [0,1,2],  2) == 0.333\n",
        "assert f1_at_k(       range(50), range(5), 30) == 0.182\n",
        "assert f1_at_k(       range(50), range(10), 30) == 0.333\n",
        "assert f1_at_k(       range(50), range(30), 30) == 0.75\n",
        "assert results_quality(range(50), range(5))  == 0.308\n",
        "assert results_quality(range(50), range(10)) == 0.5\n",
        "assert results_quality(range(50), range(30)) == 0.857\n",
        "assert results_quality(range(50), [-1]*5 + list(range(5,30))) == 0.0\n"
      ],
      "metadata": {
        "id": "HoeSnxbSoj9X"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from time import time\n",
        "\n",
        "for title_factor in [10.5]:\n",
        "  for page_rank_factor in [0.01]:\n",
        "    qs_res = []\n",
        "    for q, true_wids in queries.items():\n",
        "      duration, ap = None, None\n",
        "      t_start = time()\n",
        "\n",
        "      res = be.question_wiki(q, True, True, 10.5, 0.01, True)\n",
        "      duration = time() - t_start\n",
        "      pred_wids, _ = zip(*res)\n",
        "      rq = results_quality(true_wids, pred_wids)\n",
        "\n",
        "      qs_res.append((q, duration, rq))\n",
        "      print(f\"{q}, {duration}, {rq}\")\n",
        "    duration_sum = 0\n",
        "    quality_sum = 0\n",
        "    for q, duration, quality in qs_res:\n",
        "      duration_sum = duration_sum+duration\n",
        "      quality_sum = quality_sum+quality\n",
        "    print(f'{title_factor}, {page_rank_factor} {quality_sum/len(qs_res)}, {duration_sum/len(qs_res)}')\n",
        "\n",
        "# duration_sum = 0\n",
        "# quality_sum = 0\n",
        "# for q, duration, quality in qs_res:\n",
        "#   duration_sum = duration_sum+duration\n",
        "#   quality_sum = quality_sum+quality\n",
        "# print(f'{title_factor}, {page_rank_factor} {quality_sum/len(qs_res)}, {duration_sum/len(qs_res)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwf-qb1UoytK",
        "outputId": "3425eb6f-e7b1-42e6-b84a-56bf530bb186"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "genetics, 1.6990165710449219, 0.453\n",
            "Who is considered the \"Father of the United States\"?, 22.046374320983887, 0.0\n",
            "economic, 0.4833407402038574, 0.555\n",
            "When was the United Nations founded?, 9.819241523742676, 0.0\n",
            "video gaming, 7.3440327644348145, 0.046\n",
            "3D printing technology, 2.4122188091278076, 0.048\n",
            "Who is the author of \"1984\"?, 5.831385850906372, 0.085\n",
            "bioinformatics, 1.0766913890838623, 0.375\n",
            "Who is known for proposing the heliocentric model of the solar system?, 18.67046284675598, 0.0\n",
            "Describe the process of water erosion., 5.804383993148804, 0.248\n",
            "When was the Berlin Wall constructed?, 4.157503128051758, 0.62\n",
            "What is the meaning of the term \"Habeas Corpus\"?, 3.6338348388671875, 0.268\n",
            "telecommunications, 0.5338804721832275, 0.334\n",
            "internet, 1.7213776111602783, 0.548\n",
            "What are the characteristics of a chemical element?, 1.4570121765136719, 0.216\n",
            "Describe the structure of a plant cell., 3.134089469909668, 0.161\n",
            "Who painted \"Starry Night\"?, 9.346466541290283, 0.129\n",
            "computer, 0.6932921409606934, 0.712\n",
            "What is the structure of the Earth's layers?, 2.281078815460205, 0.135\n",
            "When did World War II end?, 24.450222969055176, 0.218\n",
            "When was the Gutenberg printing press invented?, 5.656801700592041, 0.48\n",
            "medicine, 0.6848978996276855, 0.44\n",
            "Describe the water cycle., 4.273233652114868, 0.33\n",
            "artificial intelligence, 0.66347336769104, 0.163\n",
            "physics, 0.6363093852996826, 0.456\n",
            "nanotechnology, 1.0014488697052002, 0.224\n",
            "When did the Black Death pandemic occur?, 10.403448820114136, 0.309\n",
            "neuroscience, 0.7061874866485596, 0.572\n",
            "snowboard, 0.6554267406463623, 0.364\n",
            "Who is the founder of modern psychology?, 5.155516862869263, 0.044\n",
            "10.5, 0.01 0.2844333333333334, 5.214421725273132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "con = Counter()\n",
        "test = be.title_refactoring([\"computer\"],con)\n",
        "print(list(con.items())[:10])\n",
        "\n",
        "test = be.title_refactoring([\"computer\"],con)\n",
        "\n",
        "print(list(con.items())[:10])\n"
      ],
      "metadata": {
        "id": "vilNgjMytWMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for bodyIndexing\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "pl = be.get_posting_list_body(\"computer\")\n",
        "\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "for i in range(10):\n",
        "  print(pl[i])\n",
        "\n",
        "print(\" \")\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n"
      ],
      "metadata": {
        "id": "gTkjmNcMbT6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for titleIndexing\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "pl = be.get_posting_list_title(\"computer\")\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "for i in range(30):\n",
        "  print(pl[i])\n",
        "\n",
        "print(\" \")\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "4MmWceA4bT4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for page rank indexing\n",
        "\n",
        "doc_id = 2428\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "print(be.get_page_rank(doc_id))\n",
        "\n",
        "print(\" \")\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "CKONLZIIvgiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for doc legth\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "print(be.get_doc_length(doc_id))\n",
        "\n",
        "print(\" \")\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "03nNo81Rvgf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a4d847-78d9-46b1-f2d0-f371dd594a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3375.0\n",
            " \n",
            "Elapsed time: 0.0002732276916503906 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for typos correction\n",
        "\n",
        "text_with_typos = \"ths is an exmple of text with typos\"\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "corrected_text = be.correct_typos(text_with_typos)\n",
        "\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "print(\" \")\n",
        "assert \"the is an example of text with typos\" ==  corrected_text\n",
        "\n",
        "# Took aprox 0.17 secs\n"
      ],
      "metadata": {
        "id": "L6Vys8FJvgWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for filter_query\n",
        "import time\n",
        "query = \"ai\"\n",
        "expected_tokens = [\"cat\", \"mat\"]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "ans =  be.filter_query_with_stemming(query)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "print(ans)\n",
        "assert expected_tokens ==  ans\n"
      ],
      "metadata": {
        "id": "dJS68VePwiOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for tf_idf\n",
        "start_time = time.time()\n",
        "ans = be.tf_idf([\"computer\"])\n",
        "elapsed_time = time.time() - start_time\n",
        "print(len(ans))\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "\n",
        "# assert len(ans) == 134110\n",
        "assert elapsed_time < 1.5\n"
      ],
      "metadata": {
        "id": "XXJM46TUwqie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for id_title\n",
        "title = be.title_id[18555]\n",
        "print(title)"
      ],
      "metadata": {
        "id": "9poT8_ZnNncg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for question_wiki\n",
        "start_time = time.time()\n",
        "\n",
        "ans = be.question_wiki(\"computer\",use_title=False, use_page_rank=False, TITLE_VALUE_FACTORING=1000)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time With multy thread: {elapsed_time} seconds\")\n",
        "\n",
        "# start_time = time.time()\n",
        "\n",
        "# ans2 = be.question_wiki(\"computer violet flower\",use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000,use_multy_thread=False)\n",
        "\n",
        "# elapsed_time = time.time() - start_time\n",
        "\n",
        "# print(f\"Elapsed time Without multy thread: {elapsed_time} seconds\")\n",
        "\n",
        "# assert ans1==ans2\n",
        "\n",
        "\n",
        "assert len(ans) == 100\n",
        "for i in ans:\n",
        "  print(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZP9ooRSwtQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_list = [\n",
        "    \"What is the history of the Roman Empire?\",\n",
        "    \"How does the human brain process language?\",\n",
        "    \"Who were the key figures in the Renaissance?\",\n",
        "    \"What are the main causes of climate change?\",\n",
        "    \"How does the Internet work?\",\n",
        "    \"What is the theory of relativity?\",\n",
        "    \"Who was Cleopatra and what was her impact on history?\",\n",
        "    \"What are the major events of World War II?\",\n",
        "    \"How does the human digestive system work?\",\n",
        "    \"Who were the Vikings and what were their achievements?\",\n",
        "    \"What is the significance of the Magna Carta?\",\n",
        "    \"How do vaccines work?\",\n",
        "    \"Who was Joan of Arc and what role did she play in history?\",\n",
        "    \"What are the major religions of the world?\",\n",
        "    \"What is the process of photosynthesis?\",\n",
        "    \"Who were the ancient Egyptians and what was their culture like?\",\n",
        "    \"What is the history of the United States Constitution?\",\n",
        "    \"How does the human immune system work?\",\n",
        "    \"Who was Alexander the Great and what were his conquests?\",\n",
        "    \"What are the major theories of evolution?\",\n",
        "    \"How does the global economy work?\",\n",
        "    \"What are the major accomplishments of the Industrial Revolution?\",\n",
        "    \"Who was Leonardo da Vinci and what were his contributions to art and science?\",\n",
        "    \"What are the major theories of the origin of life on Earth?\",\n",
        "    \"How does the human respiratory system work?\",\n",
        "    \"Who were the key figures in the Civil Rights Movement?\",\n",
        "    \"What is the history of the Olympic Games?\",\n",
        "    \"How does the human nervous system work?\",\n",
        "    \"Who was Julius Caesar and what were his accomplishments?\",\n",
        "    \"What are the major branches of philosophy?\",\n",
        "    \"How does the human reproductive system work?\",\n",
        "    \"Who were the key figures in the American Revolution?\",\n",
        "    \"What is the history of the space race?\",\n",
        "    \"How does the human cardiovascular system work?\",\n",
        "    \"Who was Genghis Khan and what was his impact on history?\",\n",
        "    \"What are the major theories of the origin of the universe?\",\n",
        "    \"How does the criminal justice system work?\",\n",
        "    \"Who were the key figures in the French Revolution?\",\n",
        "    \"What is the history of the United Nations?\",\n",
        "    \"How does the human endocrine system work?\",\n",
        "    \"Who was Attila the Hun and what was his impact on history?\",\n",
        "    \"What are the major theories of the formation of the solar system?\",\n",
        "    \"How does the human skeletal system work?\",\n",
        "    \"Who were the key figures in the women's suffrage movement?\",\n",
        "    \"What is the history of the European Union?\",\n",
        "    \"How does the human muscular system work?\",\n",
        "    \"Who was Napoleon Bonaparte and what were his accomplishments?\",\n",
        "    \"What are the major theories of the extinction of the dinosaurs?\",\n",
        "    \"How does the human lymphatic system work?\",\n",
        "    \"Who were the key figures in the abolitionist movement?\"\n",
        "]"
      ],
      "metadata": {
        "id": "_pP5GSFhaOhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_no_thread = []\n",
        "ans_with_thread = []"
      ],
      "metadata": {
        "id": "uZUb1MfjbD-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in questions_list:\n",
        "  start_time = time.time()\n",
        "  ans1 = be.question_wiki(q,use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  ans_no_thread.append(elapsed_time)\n",
        "  start_time = time.time()\n",
        "  ans2 = be.question_wiki(q,use_title=True, use_page_rank=True, TITLE_VALUE_FACTORING=1000,use_multy_thread=False)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  ans_with_thread.append(elapsed_time)\n"
      ],
      "metadata": {
        "id": "G1U2AIJmbD4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_no_thread"
      ],
      "metadata": {
        "id": "b26TNyZobhA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_with_thread"
      ],
      "metadata": {
        "id": "ju1ArESnbjAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have two lists of results, result1 and result2, and question_indices as before\n",
        "result1 = ans_no_thread\n",
        "result2 = ans_with_thread\n",
        "\n",
        "# Create a figure and axis with a wider size\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "\n",
        "# Define bar width and indices for the two sets of results\n",
        "bar_width = 0.35\n",
        "indices = np.arange(len(questions_list))\n",
        "question_lengths = [len(q.split()) for q in questions_list]  # Split question and count words\n",
        "\n",
        "# Create grouped bar plot\n",
        "bar1 = ax.bar(indices - bar_width/2, result1, bar_width, label='Single thread', color='b')\n",
        "bar2 = ax.bar(indices + bar_width/2, result2, bar_width, label='Multy thread', color='g')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Number of Words in Question')\n",
        "ax.set_ylabel('Time (s)')\n",
        "ax.set_title('Time Taken to Process Questions of Different Word Counts')\n",
        "\n",
        "# Set x-axis ticks and labels\n",
        "ax.set_xticks(indices)\n",
        "ax.set_xticklabels(question_lengths)\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6RVxaZ4Sf8lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming ans_no_thread and ans_with_thread are the two lists of values\n",
        "def avg_percentage_difference(list1, list2):\n",
        "    differences = [(a - b) / ((a + b) / 2) * 100 for a, b in zip(list1, list2)]\n",
        "    return sum(differences) / len(differences)\n",
        "\n",
        "avg_diff = avg_percentage_difference(ans_no_thread, ans_with_thread)\n",
        "print(f\"Average Percentage Difference: {avg_diff:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcRmjRE-jbLx",
        "outputId": "ee61cd1c-266a-45e9-b4ff-457f05a07f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Percentage Difference: -37.12%\n"
          ]
        }
      ]
    }
  ]
}